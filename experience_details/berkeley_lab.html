<!DOCTYPE html>
<html lang="en">
<head>
    <!--import files + libraries-->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/experience_details/exp.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/remixicon/fonts/remixicon.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/devicon@latest/devicon.min.css">
    <title>Berkeley Lab | ML Intern</title>
</head>
<body> 
    <!-- Keep same header nav bar as home page -->
    <header id="header">
        <nav class="navbar">
            <a href="/index.html" class="nav-branding">
                <img src="/images/Portfolio_logo.png" alt="Logo" class="nav-logo">
            </a>
            
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="/index.html#about" class="nav-link">About me</a>
                </li>
                <li class="nav-item">
                    <a href="/index.html#experience" class="nav-link">Experience</a>
                </li>
                <li class="nav-item">
                    <a href="/index.html#projects" class="nav-link">Projects</a>
                </li>
                <li class="nav-item">
                    <a href="images\Daniel_Coblentz_resume.pdf" target="_blank" class="nav-link">Resume</a>
                </li>
                <div class="dropdown-footer">
                    <hr>
                    <div class="footer-text">
                        <a href="mailto:Dan@thecoblentzs.com" class="footer-link">Email</a>
                        <a href="https://github.com/danielcoblentz" class="footer-link">GitHub</a>
                        <a href="https://www.linkedin.com/in/danielcoblentz/" class="footer-link">LinkedIn</a>
                    </div>
                </div>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </nav>
    </header>
   <!-- Define the left side nav bar -->
<nav class="experience-nav" role="navigation" aria-label="Contents">
    <span role="heading" aria-level="2" class="experience-nav__heading">Contents</span>
    <ul class="experience-nav__list">
        <li><a class="experience-nav__link" href="#overview">Overview</a></li>
        <li><a class="experience-nav__link" href="#Late-fusion">Late fusion</a></li>
        <li><a class="experience-nav__link" href="#methods">Methods</a></li>
        <li><a class="experience-nav__link sub-tab" href="#transformers">Transformers</a></li>
        <li><a class="experience-nav__link sub-tab" href="#fusion-strategy">Fusion strategy</a></li>
        <li><a class="experience-nav__link sub-tab" href="#KANS">Kolmogorov Arnald Netowrks</a></li>
        
        <li><a class="experience-nav__link" href="#experiments">Experiments</a></li>
        <li><a class="experience-nav__link" href="#evaluation">Evaluation Results</a></li>
        <li><a class="experience-nav__link" href="#future-work">Future work</a></li>
        <li><a class="experience-nav__link" href="#PyTorch">PyTorch</a></li>
    </ul>
</nav>

    <!-- Main content for info -->
    <section class="tech-doc-content">
        
        <!-- Overview Section -->
        <section id="overview" class="fade-section experience-section">
            <!-- title -->
            <h2>Lawrence Berkeley National Laboratory</h2>
            <!-- image -->
            <img src="/images/14_BL_Alt_Stack_Rev.png" alt="TA" class="ta-image"> <!--replace-->
            <!-- categories -->
            <div class="categories-container">
                <div class="categories-left">
                    <h4>Role</h4>
                    <p>Machine Learning Research Intern</p>
                    <h4>Skills</h4>
                    <p>Multimodal Machine Learning, Deep Learning, NLP</p>
                </div>
                <div class="categories-right">
                    <h4>Timeline</h4>
                    <p>May 2025 – August 2025</p>

                    <h4>Tools</h4>
                    <p>PyTorch</p>
                    <p>scikit-learn</p>
                    <p>Pandas</p>
                    <p>Regex/NLTK</p>
                    
                </div>
            </div>
            <div class="experience-description">
                <h3>Overview</h3>
                <p>
                    The Applied Mathematics & Computational Research Division <a href="https://crd.lbl.gov/divisions/amcr/" target="blank">(AMCR)</a> at Lawrence Berkeley National Laboratory advances research in mathematical modeling, algorithm design, and high-performance computing to address pressing scientific challenges. Our team applies machine learning to improve clinical phenotyping, with a focus on obstructive sleep apnea (OSA) and its related health risks.
                </p>
                <p>
                    Building on prior work that analyzed only structured medical data such as lab results, diagnoses, and medication histories, this year’s research investigated whether incorporating unstructured clinical notes could yield richer patient profiles and improve subgroup identification. Clinical notes often capture nuanced details that structured fields cannot, including symptom descriptions, lifestyle factors, and physician impressions. By combining these complementary data sources, we aimed to uncover subtle patterns in patient health trajectories and improve predictive modeling.
                </p>
                <p>
                    In my role as a Machine Learning Research Intern, I was primarily responcible for developing a late fusion pipeline to process each data type independently before combining their learned representations for analysis. 
                    My final report can be found <a href="/images/VFP Student_Summer 2025_Research Report Paper_Coblentz_Daniel.pdf">here!</a>
                </p>
            </div>
            

             <!-- Late fusion -->
        <section id="Late-fusion" class="experience-description">
            <h3>Late fusion</h3>
            <p>
                <a href="https://www.geeksforgeeks.org/deep-learning/early-fusion-vs-late-fusion-in-multimodal-data-processing/#understanding-late-fusion"> Late fusion</a> also known as decision-level fusion—is a strategy for integrating information from different types of data. Instead of feeding all features into a single model from the start, we train two separate machine learning models, each specialized for its own data modality. In our case, one model was designed for structured data, and the other for unstructured text.
            </p>

            <p>Each model learns to represent its modality independently, capturing patterns that are unique to that data type. At the fusion stage, the output embeddings from both models are combined to create a joint multimodal representation. This allows the system to leverage the strengths of each modality while avoiding the interference that can occur when mixing raw features too early.</p>
        </section>

        <!-- methods section -->
        <section id="methods" class="experience-description">
            <h3>Methods</h3>
            <p>
             Each modality followed its own preprocessing and modeling path, using architectures tailored to its data type. Structured variables were modeled with a feature-attention architecture (TabNet), while unstructured clinical text was processed with a domain-specific language model (ClinicalBERT). The resulting embeddings were then combined using a late fusion strategy to support downstream clustering and classification tasks.
            </p>
            <img src="/images/project_architecture.png" alt="TA" class="ta-image">
            <p>FIG. 1. Structured data and clinical notes are encoded separately using TabNet and ClinicalBERT. Their embeddings are fused for multimodal classification and clustering to identify patient subgroups and disease phenotypes. </p>

        </section>

        <!-- transformers (tabnet and clinicalBert) -->
        <!-- transformers (TabNet and ClinicalBERT) -->
<section id="transformers" class="experience-subsection">
    <h4>Transformers</h4>
    <p>
        A core part of this project involved applying transformer-based models to each type of data. 
        Transformers are a class of neural networks originally developed for natural language processing and now widely adopted in 
        fields such as medical imaging, speech recognition, and time-series analysis. Their strength lies in modeling complex 
        dependencies within sequential data, making them well-suited for both structured clinical features and medical notes. 
    </p>
    
    <p>
        For structured data (lab results, diagnosis codes, etc) I used <a href="https://arxiv.org/abs/1908.07442" target="_blank">TabNet</a>
        Unlike conventional neural networks that treat all features equally, TabNet applies a sequential attention mechanism to 
        selectively focus on the most relevant variables at each decision step. This improves predictive performance while also 
        offering interpretability by highlighting which features drive model decisions. In healthcare, this transparency is especially 
        important because there are many features that contribute to a patient profile but not all of them have equal importance.
        
    </p>
    
    <p>
        For unstructured text, such as discharge summaries and physician notes, I used  <a href="https://arxiv.org/abs/1904.05342" target="_blank">ClinicalBERT</a>, a domain-specific 
        variant of the BERT transformer model. Trained on large corpora of biomedical literature and clinical notes, ClinicalBERT 
        captures the nuances of medical terminology, abbreviations, and context-specific language that general-purpose models often 
        miss. In this project, ClinicalBERT transformed narrative records into dense vector representations, encoding both clinical 
        meaning and context. 
    </p>
</section>


       <!-- Fusion strategy -->
<!-- Fusion strategy -->
<section id="fusion-strategy" class="experience-subsection">
    <h4>Fusion Strategy</h4>
    <p>
        Once each modality was processed by its respective model, the outputs were transformed into 
        <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank">embeddings</a>—dense numerical vectors that 
        summarize complex information in a compact form. For structured data, TabNet produced 128-dimensional embeddings 
        that captured key clinical features. For unstructured text, ClinicalBERT generated 768-dimensional embeddings 
        from discharge summaries, which were reduced to 128 dimensions for consistency across modalities.
    </p>
    
    <p>
        These embeddings function as “compressed snapshots” of a patient’s profile, where similar patients map closer 
        together in this high-dimensional space. To integrate them, we adopted a 
        <a href="https://www.geeksforgeeks.org/deep-learning/early-fusion-vs-late-fusion-in-multimodal-data-processing/#understanding-late-fusion" target="_blank">late fusion</a> 
        strategy—concatenating embeddings from each modality and passing the combined representation through fully connected 
        layers for joint optimization. This approach preserved the strengths of both data types while enabling downstream 
        clustering, classification, and visualization within a unified latent space.
    </p>
    
    <p>
        We also explored contrastive learning as an alternative fusion strategy, projecting embeddings into a shared latent space and training them to align for the same patient. However, this approach did not yield stable performance on our dataset and was ultimately not used in the final pipeline.
    </p>
</section>


        


       <!-- Kolmogorov-Arnold Networks -->
<section id="KANS" class="experience-subsection">
    <h4>Kolmogorov-Arnold Networks (KANs)</h4>
    <p>
        Kolmogorov-Arnold Networks are a relatively new neural network architecture inspired by the 
        <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem" target="_blank">
        Kolmogorov–Arnold representation theorem</a>, which states that any multivariate continuous function can be expressed 
        as a composition of single-variable functions. Unlike traditional multilayer perceptrons (MLPs) that use fixed activation 
        functions at each neuron, KANs place learnable non-linear functions—often modeled with 
        <a href="https://en.wikipedia.org/wiki/B-spline" target="_blank">B-splines</a>—on the edges between nodes. 
        This design makes them more expressive and interpretable, while often requiring fewer parameters.
    </p> 
    
    <p>
        In our project, KANs were tested as part of the <strong>classification pipeline</strong>, one of our key downstream tasks. 
        Their ability to capture complex interactions in structured clinical data made them a strong candidate for comparison 
        against traditional models such as <strong>Random Forests</strong> and <strong>Logistic Regression</strong>. 
        While KANs required longer training times, they demonstrated promising representational power, and with recent optimizations 
        (e.g., <a href="https://github.com/Blealtan/efficient-kan" target="_blank">efficient-KAN</a>), these limitations are becoming less significant. 
        This suggests KANs could play an important role in advancing future healthcare AI applications.
    </p>

    <img src="/images/KAN.png" alt="TA" class="ta-image">
    <p>Fig 2. Kolmogorov-Arnold Network architecture compared to a traditional MLP.</p>
</section>


<section id="experiments" class="experience-subsection">
    <h4>Experiments</h4>
    <p>
      We evaluated both single-modality and multimodal setups for patient profiling. Baselines were trained
      independently on structured features (<strong>TabNet</strong>) and unstructured discharge summaries
      (<strong>ClinicalBERT</strong>) to establish modality-specific performance. For multimodal learning,
      we primarily used <strong>late fusion</strong>—concatenating modality embeddings (128&nbsp;dims each)
      and training a joint head. We also explored <strong>contrastive learning</strong> to align
      cross-modal pairs in a shared space; however, it was unstable on our data and was not used in the final pipeline.
    </p>
    <p>
      Downstream classification used <strong>Kolmogorov–Arnold Networks (KANs)</strong>,
      <strong>Random Forests</strong>, and <strong>Logistic Regression</strong> on frozen embeddings.
      Hyperparameters were tuned with cross-validation and fixed seeds. Performance was measured on two tasks:
      (1) <em>classification</em> of comorbidity risk (AUROC, accuracy) and
      (2) <em>clustering</em> for subgroup discovery (Silhouette, Adjusted Rand Index, Davies–Bouldin).
      This dual evaluation captures both predictive utility and the structure of learned patient subgroups.
    </p>
  </section>
  

        

  <section id="evaluation" class="experience-subsection">
    <h4>Evaluation</h4>
    <p>
        To assess the quality of the learned multimodal patient representations, we evaluated both clustering and classification performance. 
        Our pipeline generated <strong>embeddings</strong>—dense vector representations of patients—using separate encoders for structured data (TabNet) 
        and unstructured text (ClinicalBERT). These embeddings were then fused in a shared latent space, providing the foundation for downstream analysis.
    </p>
    <p>
        For clustering evaluation, we computed the <strong>Silhouette Score</strong> to measure intra-cluster cohesion and inter-cluster separation, 
        along with the <strong>Adjusted Rand Index (ARI)</strong> and <strong>Davies–Bouldin Index (DBI)</strong>. 
        Visualizations such as UMAP projections and cluster heatmaps were also used to qualitatively inspect subgroup separation and highlight 
        potential clinical patterns.
    </p>
    <p>
        To further validate the embeddings, we applied them to a supervised classification task. Using models such as <strong>Random Forests</strong> 
        and <strong>Kolmogorov-Arnold Networks (KANs)</strong>, we measured predictive performance with the <strong>Area Under the Receiver Operating Characteristic (AUROC)</strong> score, 
        providing a robust metric for distinguishing between clinical outcomes.
    </p>
    <p>
        Together, these quantitative metrics and qualitative visual analyses offered a comprehensive evaluation of our multimodal learning pipeline—capturing both the structural quality of discovered subgroups and their predictive relevance for downstream tasks.
    </p>

    <img src="/images/LBNL_results_table.png" alt="TA" class="ta-image">
    <p>
        Table I. Clustering performance comparison across data modalities. Structured, unstructured, and fused embeddings 
        (ARI: -1 to 1, Silhouette: -1 to 1, DBI: 0 to ∞; higher ARI and Silhouette values indicate better clustering, lower DBI is favorable).
    </p>
</section>

        

        <!--future work -->
        <section id="future-work" class="experience-subsection">
            <h4>Future Work</h4>
            <p>There are several directions to extend and refine this work:</p>
            <ol>
                <li>1) Incorporating Additional Modalities: Expanding the input space to include imaging data, time-series vital signs, or genetic information could further enhance patient stratification.</li>
                <li>2) Exploring Alternative Fusion Strategies: While this study employed late fusion, other strategies such as hybrid or hierarchical fusion could be tested to capture different types of cross-modal interactions.</li>
                <li>3) Temporal Modeling of Patient Data: Introducing models that explicitly capture longitudinal changes—such as Transformer-based time-series architectures—could improve the detection of evolving patient risk profiles.</li>
                <li>4) Fine-Tuning on Downstream Clinical Tasks: Applying the learned multimodal embeddings to predictive tasks (e.g., readmission risk, mortality prediction) would provide further validation of their utility.</li>
                <li>5) Integrating Clinician Feedback: Building interactive tools for physicians to explore and validate clusters could bridge the gap between model outputs and clinical decision-making.</li>
            </ol>
            <p>By pursuing these avenues, future research can not only refine technical performance but also increase the translational impact of multimodal deep phenotyping in precision healthcare.</p>
        </section>
        

    <!-- tools / reflections -->
<section id="PyTorch" class="experience-description">
    <h3>PyTorch</h3>
    <p>
        <a href="https://pytorch.org/" target="_blank">PyTorch</a>, developed by Meta AI Research, 
        is one of the most widely used deep learning frameworks. 
        It provides two core capabilities:
    </p>
    <ul>
        <li><strong>Tensor computation</strong> (similar to NumPy) with GPU acceleration.</li>
        <li><strong>Deep neural networks</strong> built on a dynamic, tape-based autograd system.</li>
    </ul>
    <p>
        Unlike frameworks that require static computation graphs, PyTorch supports 
        <strong>dynamic computational graphs</strong>, allowing models to adapt at runtime. 
        This flexibility makes it especially useful for research and experimentation in areas like 
        image recognition, natural language processing, and time-series modeling.
    </p>
    <p>
        For this project, PyTorch served as the backbone for training both <strong>TabNet</strong> and 
        <strong>ClinicalBERT</strong>, enabling us to efficiently build and test multimodal fusion pipelines. 
        I’ve also used PyTorch in personal projects, where its flexibility and strong ecosystem 
        (alongside libraries like <strong>scikit-learn</strong> and <strong>TensorFlow</strong>) 
        allow me to explore ideas quickly and translate them into working prototypes.
    </p>
</section>



            
<!--Footer content----------------------------------------------------->
<footer class="site-footer">
    <p>&copy; 2024 Daniel Coblentz. All rights reserved.</p>
    <div class="footer-icons">
        <a href="mailto:Dan@thecoblentzs.com" target="_blank"><i class="ri-mail-line"></i></a>
        <a href="https://github.com/danielcoblentz" target="_blank"><i class="ri-github-line"></i></a>
        <a href="https://www.linkedin.com/in/danielcoblentz/" target="_blank"><i class="devicon-linkedin-plain"></i></a>
    </div>
</footer>
    <!-- Link to JS for header functions -->
    <script src="/experience_details/exp.js"></script>
    <script src="/script.js"></script>
</body>
</html>
