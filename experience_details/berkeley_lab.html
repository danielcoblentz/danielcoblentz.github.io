<!DOCTYPE html>
<html lang="en">
<head>
    <!--import files + libraries-->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/experience_details/exp.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/remixicon/fonts/remixicon.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/devicon@latest/devicon.min.css">
    <title>Berkeley Lab | ML Intern</title>
</head>
<body> 
    <!-- Keep same header nav bar as home page -->
    <header id="header">
        <nav class="navbar">
            <a href="/index.html" class="nav-branding">
                <img src="/images/Portfolio_logo.png" alt="Logo" class="nav-logo">
            </a>
            
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="/index.html#about" class="nav-link">About me</a>
                </li>
                <li class="nav-item">
                    <a href="/index.html#experience" class="nav-link">Experience</a>
                </li>
                <li class="nav-item">
                    <a href="/index.html#projects" class="nav-link">Projects</a>
                </li>
                <li class="nav-item">
                    <a href="images\Daniel_Coblentz_resume.pdf" target="_blank" class="nav-link">Resume</a>
                </li>
                <div class="dropdown-footer">
                    <hr>
                    <div class="footer-text">
                        <a href="mailto:Dan@thecoblentzs.com" class="footer-link">Email</a>
                        <a href="https://github.com/danielcoblentz" class="footer-link">GitHub</a>
                        <a href="https://www.linkedin.com/in/danielcoblentz/" class="footer-link">LinkedIn</a>
                    </div>
                </div>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </nav>
    </header>
   <!-- Define the left side nav bar -->
<nav class="experience-nav" role="navigation" aria-label="Contents">
    <span role="heading" aria-level="2" class="experience-nav__heading">Contents</span>
    <ul class="experience-nav__list">
        <li><a class="experience-nav__link" href="#overview">Overview</a></li>
        <li><a class="experience-nav__link" href="#Project-Architecture">Project Architecture</a></li>
        <li><a class="experience-nav__link" href="#processing">Data Processing</a></li>
        <li><a class="experience-nav__link sub-tab" href="#structured-data">Structured Data</a></li>
        <li><a class="experience-nav__link sub-tab" href="#unstructured-data">Unstructured Data</a></li>
        <li><a class="experience-nav__link" href="#fusion">Multimodal Fusion</a></li>
        
        <li><a class="experience-nav__link" href="#experiments">Experimental Methods</a></li>
        <li><a class="experience-nav__link sub-tab" href="#kan">Kolmogorov-Arnold Networks</a></li>
        <li><a class="experience-nav__link sub-tab" href="#contrastive-learning">Contrastive Learning</a></li>
        <li><a class="experience-nav__link" href="#results">Results & Evaluation</a></li>
        <li><a class="experience-nav__link" href="#future-work">Future Work</a></li>
        <li><a class="experience-nav__link" href="#PyTorch">PyTorch</a></li>
    </ul>
</nav>

    <!-- Main content for info -->
    <section class="tech-doc-content">
        
        <!-- Overview Section -->
        <section id="overview" class="fade-section experience-section">
            <!-- title -->
            <h2>Lawrence Berkeley National Laboratory</h2>
            <!-- image -->
            <img src="/images/Berkeley_Lab_Logo_Large.png" alt="TA" class="ta-image"> <!--replace-->
            <!-- categories -->
            <div class="categories-container">
                <div class="categories-left">
                    <h4>Role</h4>
                    <p>Machine Learning Research Intern</p>
                    <h4>Skills</h4>
                    <p>Multimodal Machine Learning, Deep Learning, Clinical NLP</p>
                </div>
                <div class="categories-right">
                    <h4>Timeline</h4>
                    <p>May 2025 – August 2025</p>

                    <h4>Tools</h4>
                    <p>PyTorch</p>
                    <p>scikit-learn</p>
                    <p>Pandas</p>
                    <p>Regex/NLTK</p>
                    
                </div>
            </div>
            <div class="experience-description">
                <h3>Overview</h3>
                <p>
                    The Applied Mathematics & Computational Research Division <a href="https://crd.lbl.gov/divisions/amcr/" target="blank">(AMCR)</a> at Lawrence Berkeley National Laboratory advances research in mathematical modeling, algorithm design, 
                    and high-performance computing to address pressing scientific challenges. I joined the Applied Computing for Scientific Discovery (ACSD) Group, where I worked in collaboration on their <a href= "https://pmc.ncbi.nlm.nih.gov/articles/PMC8065854/">clinical phenotyping</a> research project focused on obstructive sleep apnea (OSA) and its related comorbidities.
                </p>
                <p>
                    Traditional clinical phenotyping relies heavily on structured medical data (lab results, diagnosis codes, and medication histories). However, 
                    clinical notes contain rich, unstructured information that often captures nuanced patient details unavailable in structured fields: symptom descriptions, lifestyle factors, physician impressions, and contextual observations. 
                    The challenge was developing a system that could effectively combine these complementary data sources to create more comprehensive patient profiles.
                </p>
                <p>
                    In my role as a Machine Learning Research Intern, I was responsible for developing a late fusion ML pipeline to process each data type independently before combining their learned representations for analysis. My complete research findings are documented
                     in my <a href= "/images/VFP Student_Summer 2025_Research Report Paper_Coblentz_Daniel.pdf">final research report.</a>
                </p>
            </div>
            

             <!-- Project-Architecture -->
        <section id="Project-Architecture" class="experience-description">
            <h3>Project Architecture</h3>
            <p>
                The core challenge in multimodal healthcare data is that structured clinical variables and unstructured text require fundamentally different processing approaches. Structured data benefits from tabular learning methods 
                that can handle mixed data types and missing values, while clinical text requires domain-specific natural language processing to capture medical terminology and context.
            </p>

            <p>
            I implemented a <a href="https://apxml.com/courses/intro-to-multimodal-ai/chapter-3-techniques-integrating-modalities/late-fusion">late fusion</a> architecture that trains separate specialized encoders for each modality before combining their outputs. This approach allows each model to learn optimal representations for its
             respective data modality without interference from the other modalities during initial training. The structured data encoder uses <a href="https://arxiv.org/pdf/1908.07442">TabNet's</a> sequential attention mechanism, while the text encoder leverages <a href="https://arxiv.org/pdf/1904.05342">ClinicalBERT's</a> domain-specific language understanding.
            </p>
            <p>
                <img src="/images/project_architecture.png" alt="TA" class="exp-image">
            </p>
            <p>
                The fusion stage concatenates 128-dimensional embeddings from each encoder, creating a unified 256-dimensional latent space representation. Embeddings are dense numerical vectors that compress complex, high-dimensional information into compact 
                representations where semantically similar data points map closer together in the vector space. This joint embedding captures both the quantitative precision of structured data and the qualitative richness of clinical narratives, enabling enhanced patient profiling compared to either modality alone.

            </p>
        </section>

        <!-- Data Processing section -->
        <section id="processing" class="experience-description">
            <h3>Data Processing</h3>
            <p>
                Machine learning systems work with two fundamentally different types of data. Structured data consists of organized information stored in predefined formats like spreadsheet tables with clear rows, columns, and standardized values. Unstructured data, in contrast, is free-form information without a predefined structure. It is primarily human-written text, images, or audio that doesn’t fit neatly into database fields. Think of social media posts, product reviews, or email content that contain insights but requires specialized processing to extract meaningful patterns.
            </p>
            <p>
                For this clinical dataset, comprehensive preprocessing was applied to prepare both data modalities for machine learning. The pipeline included removing low-variance features to address sparsity, applying one-hot encoding and standard scaling transformations, and reducing features through statistical analysis. SHAP (SHapley Additive exPlanations) analysis was used to highlight the most informative variables, ensuring that only high-quality features were retained. Additional steps involved handling missing values, log transformations for skewed distributions, and specialized medication encoding strategies that incorporated both binary presence and count-based representations to optimize feature quality.
            </p>
            

        </section>

        <!-- structured data sub section -->
<section id="structured-data" class="experience-subsection">
    <h4>Structured Data</h4>
    <p>
        Clinical structured data presents unique challenges: high dimensionality, mixed data types, and complex feature interactions. Traditional <a href="https://www.ibm.com/think/topics/neural-networks">neural networks</a> treat all input features equally, but in healthcare, 
        feature importance varies dramatically across patients and conditions. Lab values, demographics, and medication histories don't contribute uniformly to every clinical decision.
    </p>
    <p>
        To address these challenges, I implemented TabNet, a transformer-based architecture specifically designed for tabular data. TabNet applies sequential attention across features, 
        learning to focus on the most relevant variables at each decision step. For our clinical dataset, I configured TabNet with 3 decision steps and a step size of 128 which yielded the best results in model training.
    </p>
    <p>
        The attention mechanism learned to prioritize features like medications and known comorbidity indicators for OSA phenotyping, while de-emphasizing less relevant demographic variables. This selective attention proved especially valuable during model interpretation, allowing clinicians to understand which patient characteristics drove specific predictions, a critical requirement in healthcare applications.
    </p>
</section>


<!-- unstructured data -->
<section id="unstructured-data" class="experience-subsection">
    <h4>Unstructured Data</h4>
    <p>
        Clinical notes contain information that structured fields cannot capture: detailed symptom descriptions, temporal relationships, physician reasoning, and contextual observations. However, medical text presents significant NLP challenges, including domain-specific terminology, frequent abbreviations, and specialized syntax that general-purpose language models struggle to interpret correctly.
    </p>
    
    <p>
        I implemented ClinicalBERT, a domain-specific variant of <a href="https://arxiv.org/pdf/1810.04805">BERT</a> pre-trained on biomedical literature and clinical notes. Unlike general-purpose language models, ClinicalBERT understands medical terminology such as "CPAP" (continuous positive airway pressure) and "AHI" (apnea-hypopnea index) that are crucial for accurate interpretation of patient notes.
    </p>
    
    <p>
        The implementation involved developing a preprocessing pipeline that tokenized clinical notes while preserving medical abbreviations and handling varying note lengths through truncation strategies optimized for BERT's 512-token input limit. ClinicalBERT generated 768-dimensional embeddings, which I compressed to 128 dimensions using a learned projection layer to match TabNet's output dimensions.
    </p>
</section>



       <!-- Multimodal fusion techniques -->
<section id="fusion" class="experience-description">
    <h4>Multimodal fusion</h4>
    <p>
        Combining structured and unstructured clinical data requires careful consideration of how and when to integrate information from each modality. The fusion strategy significantly impacts the model's ability to leverage complementary information while avoiding interference between different data types.
    </p> 
    
    <p>
        There are multiple fusion strategies most common are <a href="https://medium.com/haileleol-tibebu/data-fusion-78e68e65b2d1">early fusion</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417424031750">knowledge graphs</a> and late fusion also known as decision-level fusion which is what I'm focusing on. Instead of feeding all features into a single model from the start (early fusion), we train two separate machine learning models, each specialized for its own data modality. In our case, one model was designed for structured data, and the other for unstructured clinical text.
    </p>

<p>Each model learns to represent its modality independently, capturing patterns that are unique to that data type. At the fusion stage, the output embeddings from both models are combined to create a joint multimodal representation. This allows the system to leverage the strengths of each modality while avoiding the interference that can occur when mixing raw features too early.

</p>
</section>


<section id="experiments" class="experience-description">
    <h3>Experimental methods</h3>
    <p>
        I designed a comprehensive evaluation framework to assess both single-modality baselines and multimodal performance across multiple tasks. The experimental methodology included exploration of various architectures and alternative fusion strategies to evaluate the multimodal approach.
    </p>
    <p>
        Baseline models were trained independently on each modality: TabNet for structured data and ClinicalBERT for clinical notes. All experiments used fixed random seeds and consistent train/validation/test splits to ensure reproducible results. Hyperparameters were optimized using grid search with 5-fold cross-validation, and performance was evaluated on held-out test sets.
    </p>
  </section>

  <!-- KANS -->
<section id="kan" class="experience-subsection">
    <h4>Kolmogorov-Arnold Networks</h4>
    <p>
        As part of the experimental framework, I explored <a href="https://arxiv.org/pdf/2404.19756">Kolmogorov-Arnold Networks (KANs)</a> for downstream classification tasks. KANs represent a relatively new neural architecture that places learnable functions on network edges rather than nodes, inspired by the Kolmogorov-Arnold representation theorem which states that any multivariate continuous function can be expressed as a composition of single-variable functions.
    </p>
    
    <p>
        Unlike traditional MLPs that use fixed activation functions, KANs employ learnable B-spline functions on edges, making them more expressive for capturing complex feature interactions. This property made them interesting for clinical data, where patient features often interact in non-linear ways. While KANs required longer training times than traditional classifiers, they demonstrated competitive performance and offered improved interpretability through visualizable edge functions.

    </p>
    <p>
        
        <img src="/images/KAN.png" alt="TA" class="exp-image"> 
    </p>
    
    
</section>

<!-- Contrastive Learning -->
<section id="contrastive-learning" class="experience-subsection">
    <h4>Contrastive Learning</h4>
    <p>
        As an alternative to other fusion methods, I experimented with contrastive learning to align cross-modal embeddings in a shared latent space. This method trains the model to bring embeddings from the same patient closer together while pushing different patients' embeddings apart, regardless of modality.
    </p>
    
    <p>
        The contrastive approach used InfoNCE loss to maximize mutual information between structured and text embeddings for the same patient. The goal was to create a unified representation space where semantically similar patients would cluster together, even when comparing across different data modalities.
    </p>
    
    <p>
        However, this method proved unstable on our clinical dataset, likely due to the high variability in clinical documentation styles and the relatively small dataset size. Training loss remained low however, the model struggled to separate positive and negative pairs in latent space. There are a number of reasons for this such as insufficient negative sampling diversity (clinical notes from different patients may still share similar medical terminology), high intra-class variance (the same patient's different notes can vary significantly in content and style), limited semantic signal in structured features for contrastive alignment, and class imbalance where certain patient conditions dominate the dataset, making it difficult to learn discriminative representations across diverse patient populations.
    </p>
    <p>The approach was ultimately not included in the final pipeline, though it remains a promising direction for future work with larger, more diverse clinical datasets that could provide the scale and variety needed for stable contrastive learning.</p>
</section>
  

        

  <section id="results" class="experience-description">
    <h3>Results & Evaluation</h3>
    <p>
        The evaluation framework assessed both clustering quality and classification performance to validate the learned patient representations. Clustering metrics measured how well the pipeline discovered meaningful patient subgroups, while classification tasks evaluated predictive utility for clinical outcomes.
    </p>
    <p>
        For clustering evaluation, I used UMAP to reduce the dimentions of each represntation into a 2D latent space plot and computed Silhouette Scores to measure cluster cohesion and separation, Adjusted Rand Index (ARI) to assess clustering agreement with clinical expectations, and Davies-Bouldin Index (DBI) to evaluate cluster compactness. These metrics provided comprehensive assessment of subgroup discovery quality.
        <img src="/images/LBNL_results_table.png" alt="classification_table" class="exp-image">
        
    </p>
    <p>
        Classification performance was measured using Area Under the Receiver Operating Characteristic curve (AUROC) for binary outcomes related to OSA complications and comorbidities with three different models Random Forest (RF), Logistic Regression (LR) and KANS. This metric effectively handles class imbalance common in clinical datasets and provides clinically interpretable performance measures.
        <img src="/images/classification_table.png" alt="results_table" class="exp-image">
        
    <p>
        The results demonstrated clear benefits from multimodal fusion. The combined approach achieved a 55% improvement in clustering quality over the best single-modality baseline and maintained competitive classification performance while discovering more clinically meaningful patient subgroups.
    </p>
</section>

        
<!--future work -->
<section id="future-work" class="experience-description">
    <h3>Future Work</h3>
    <p>
        This multimodal clinical phenotyping pipeline opens several promising research directions 
        that could further enhance patient stratification and clinical decision-making capabilities.
    </p>
    <p>
        Expanding multimodal coverage:Incorporating additional clinical modalities such 
        as medical imaging (CT scans, X-rays), time-series physiological data (continuous vital signs, ECGs), 
        and genomic information could create richer patient representations. 
        Each modality would need its own encoder, which the late fusion framework can readily accommodate.
    </p>
    <p>
        Exploring advanced fusion strategies:While late fusion proved effective, 
        hierarchical fusion or attention-based mechanisms could better capture cross-modal interactions. 
        Graph neural networks may also model relationships between clinical entities across modalities, 
        potentially improving subgroup discovery.
    </p>
    <p>
        Improving clinical text filtering: Currently, all clinical notes are included 
        without prioritization. Identifying and emphasizing semantically rich text could enhance 
        representation quality and lead to more robust predictions.
    </p>
</section>

        

<!-- tools / reflections -->
<section id="PyTorch" class="experience-description">
    <h3>PyTorch</h3>
    <p>
        <a href="https://pytorch.org/" target="_blank">PyTorch</a>, developed by Meta AI Research, 
        is one of the most widely used deep learning frameworks. It combines efficient tensor computation 
        with GPU acceleration and a dynamic autograd system for building deep neural networks. Unlike 
        frameworks that rely on static computation graphs, PyTorch allows models to be defined and 
        modified at runtime, giving researchers and developers a high degree of flexibility in experimentation. 
        This adaptability has made it especially popular in areas such as image recognition, natural language 
        processing, and time-series modeling.
    </p>
    <p>
        In my work, PyTorch served as the foundation for training both TabNetand 
        ClinicalBERT, enabling fast prototyping and efficient multimodal fusion pipelines. 
        I also use PyTorch in personal projects, where its intuitive design and large ecosystem, often 
        integrated alongside libraries like scikit-learn and TensorFlow, 
        make it an excellent choice for testing new ideas and quickly turning them into working prototypes.
    </p>
</section>

            
<!--Footer content----------------------------------------------------->
<footer class="site-footer">
    <p>&copy; 2024 Daniel Coblentz. All rights reserved.</p>
    <div class="footer-icons">
        <a href="mailto:Dan@thecoblentzs.com" target="_blank"><i class="ri-mail-line"></i></a>
        <a href="https://github.com/danielcoblentz" target="_blank"><i class="ri-github-line"></i></a>
        <a href="https://www.linkedin.com/in/danielcoblentz/" target="_blank"><i class="devicon-linkedin-plain"></i></a>
    </div>
</footer>
    <!-- Link to JS for header functions -->
    <script src="/experience_details/exp.js"></script>
    <script src="/script.js"></script>
</body>
</html>
